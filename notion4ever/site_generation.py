import json
import logging
import shutil
from pathlib import Path
from urllib.parse import urljoin

import dateutil.parser as dt_parser
import jinja2
import markdown
import sass

from notion4ever.structuring import clean_url_string

import os
import re
from pathlib import Path
from urllib.parse import quote

_WIN_ABS = re.compile(r"^[a-zA-Z]:[\\/]")
_POSIX_ABS = re.compile(r"^/")

def _as_url_path(p: str) -> str:
    # filesystem -> url (—Å–ª—ç—à–∏ + –±–µ–∑–æ–ø–∞—Å–Ω—ã–π url-encode)
    p = p.replace("\\", "/")
    return quote(p, safe="/:._-~")

def _strip_output_dir_prefix(target: str, output_dir: Path) -> str:
    """
    –ï—Å–ª–∏ target —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç output_dir (–∞–±—Å–æ–ª—é—Ç–Ω–æ –∏–ª–∏ –∫–∞–∫ —Ö–≤–æ—Å—Ç), –æ—Ç—Ä–µ–∑–∞–µ–º –µ–≥–æ.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å –í–ù–£–¢–†–ò output_dir.
    """
    t = str(target).replace("\\", "/").strip()
    out = str(output_dir.resolve()).replace("\\", "/").rstrip("/")

    # abs: C:/.../_site/root_x/download.png -> root_x/download.png (–µ—Å–ª–∏ output_dir=_site)
    if t.startswith(out + "/"):
        return t[len(out) + 1 :]

    # fallback: –µ—Å–ª–∏ –≥–¥–µ-—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è "/<output_dir.name>/"
    needle = "/" + output_dir.name.strip("/\\") + "/"
    pos = t.find(needle)
    if pos != -1:
        return t[pos + len(needle) :]

    return t

def to_rel_url(from_html_path: Path, target: str | None, output_dir: Path) -> str | None:
    """
    –î–µ–ª–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –æ—Ç html —Ñ–∞–π–ª–∞ –¥–æ target.
    –†–∞–±–æ—Ç–∞–µ—Ç –∏ –Ω–∞ Windows, –∏ –Ω–∞ Linux.
    """
    if not target:
        return target

    s = str(target).strip()

    # remote/data ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
    if s.startswith(("http://", "https://", "data:")):
        return s

    out_dir = output_dir.resolve()
    html_dir = from_html_path.parent.resolve()

    # –µ—Å–ª–∏ target —Å–æ–¥–µ—Ä–∂–∏—Ç output_dir ‚Äî –æ—Ç—Ä–µ–∂–µ–º
    s2 = _strip_output_dir_prefix(s, out_dir)

    # –µ—Å–ª–∏ target –∞–±—Å–æ–ª—é—Ç–Ω—ã–π FS –ø—É—Ç—å ‚Äî relpath –æ—Ç html_dir
    if _WIN_ABS.match(s2) or _POSIX_ABS.match(s2) or Path(s2).is_absolute():
        rel = os.path.relpath(s2, start=str(html_dir))
        return _as_url_path(rel)

    # –∏–Ω–∞—á–µ —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –ø—É—Ç—å –≤–Ω—É—Ç—Ä–∏ output_dir
    fs_target = (out_dir / s2.lstrip("/")).resolve()
    rel = os.path.relpath(str(fs_target), start=str(html_dir))
    return _as_url_path(rel)

def rewrite_abs_src_href(html: str, html_path: Path, output_dir: Path) -> str:
    """
    –ß–∏–Ω–∏—Ç src/href –≤ html_content, –µ—Å–ª–∏ markdown->html —É–∂–µ –≤—Å—Ç–∞–≤–∏–ª –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ FS –ø—É—Ç–∏.
    """
    def repl(m):
        attr = m.group(1)
        url = m.group(2)
        fixed = to_rel_url(html_path, url, output_dir)
        return f'{attr}="{fixed}"'

    return re.sub(r'(src|href)\s*=\s*"([^"]+)"', repl, html)



def verify_templates(config: dict):
    """Verifies existense and content of sass and templates dirs."""
    sass_dir = Path(config["sass_dir"])
    templates_dir = Path(config["templates_dir"])

    if sass_dir.is_dir() and any(sass_dir.iterdir()):
        logging.debug("ü§ñ Sass directory is OK")
    else:
        logging.critical("ü§ñ Sass directory is not found or empty.")

    if templates_dir.is_dir() and any(templates_dir.iterdir()):
        logging.debug("ü§ñ Templates directory is OK")
    else:
        logging.critical("ü§ñ Templates directory is not found or empty.")


def generate_css(config: dict):
    """Generates css file (compiling sass files in the output_dir folder)."""
    out_css = Path(config["output_dir"]) / "css"
    out_css.mkdir(parents=True, exist_ok=True)
    sass.compile(dirname=(config["sass_dir"], out_css))


def generate_404(structured_notion: dict, config: dict):
    """Generates 404 html page."""
    out_dir = Path(config["output_dir"]).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    tml = (Path(config["templates_dir"]) / "404.html").read_text(encoding="utf-8")
    jinja_loader = jinja2.FileSystemLoader(config["templates_dir"])
    jtml = jinja2.Environment(loader=jinja_loader).from_string(tml)
    html_page = jtml.render(content="", site=structured_notion)

    path_404 = out_dir / "404.html"
    path_404.parent.mkdir(parents=True, exist_ok=True)
    with open(path_404, "w+", encoding="utf-8") as f:
        f.write(html_page)


def generate_archive(structured_notion: dict, config: dict):
    """Generates archive page."""
    out_dir = Path(config["output_dir"]).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    if config["build_locally"]:
        archive_link = "Archive.html"
        structured_notion["archive_url"] = str(out_dir / archive_link)
        archive_path = out_dir / archive_link
    else:
        archive_link = "Archive/index.html"
        structured_notion["archive_url"] = urljoin(structured_notion["base_url"], archive_link)
        archive_path = out_dir / "Archive" / "index.html"

    archive_path.parent.mkdir(parents=True, exist_ok=True)

    tml = (Path(config["templates_dir"]) / "archive.html").read_text(encoding="utf-8")
    jinja_loader = jinja2.FileSystemLoader(config["templates_dir"])
    jtemplate = jinja2.Environment(loader=jinja_loader).from_string(tml)
    html_page = jtemplate.render(content="", site=structured_notion)

    with open(archive_path, "w+", encoding="utf-8") as f:
        f.write(html_page)


def str_to_dt(structured_notion: dict):
    for page_id, page in structured_notion["pages"].items():
        for field in ["date", "date_end", "last_edited_time"]:
            if field in page:
                structured_notion["pages"][page_id][field] = dt_parser.isoparse(page[field])


def generate_page(page_id: str, structured_notion: dict, config: dict):
    page = structured_notion["pages"][page_id]
    page_url = page["url"]

    # ‚úÖ –°–µ–π–≤–∏–º md-–∏–º—è –æ—Ç Windows/URL-—Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—É—Å—Ç—ã—Ö —Ç–∞–π—Ç–ª–æ–≤
    md_filename = clean_url_string(page.get("title"), fallback=f"untitled_{page_id[:8]}") + ".md"

    output_dir = Path(config["output_dir"]).resolve()
    output_dir.mkdir(parents=True, exist_ok=True)

    # page["url"] –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ ‚Äî –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ HTML-—Ñ–∞–π–ª—É
    page_path = Path(page_url)

    # –ò–Ω–æ–≥–¥–∞ page_url –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–∞–Ω–Ω—ã–º ‚Äî —Å—Ç—Ä–∞—Ö—É–µ–º—Å—è
    try:
        folder_path = page_path.parent
    except Exception:
        folder_path = output_dir

    try:
        rel_folder = folder_path.relative_to(output_dir)
    except ValueError:
        rel_folder = Path(".")

    local_file_location = str(rel_folder)
    html_filename = clean_url_string(page_path.name, fallback="index")  # –Ω–∞ –≤—Å—è–∫–∏–π

    logging.debug(
        f"ü§ñ MD {Path(local_file_location) / md_filename}; "
        f"HTML {Path(local_file_location) / html_filename}"
    )

    base_dir = (output_dir / Path(local_file_location)).resolve()
    base_dir.mkdir(parents=True, exist_ok=True)

    # ‚úÖ Markdown
    md_path = (base_dir / md_filename).resolve()
    md_path.parent.mkdir(parents=True, exist_ok=True)

    with open(md_path, "w+", encoding="utf-8") as f:
        metadata = (
            "---\n"
            f"title: {page.get('title')}\n"
            f"cover: {page.get('cover')}\n"
            f"icon: {page.get('icon')}\n"
            f"emoji: {page.get('emoji')}\n"
        )

        if "properties_md" in page:
            for p_title, p_md in page["properties_md"].items():
                metadata += f"{p_title}: {p_md}\n"

        metadata += "---\n\n"

        md_content = metadata + (page.get("md_content") or "")
        f.write(md_content)

    # ‚úÖ HTML
    html_content = markdown.markdown(
        md_content,
        extensions=[
            "meta",
            "tables",
            "mdx_truly_sane_lists",
            "markdown_captions",
            "pymdownx.tilde",
            "pymdownx.tasklist",
            "pymdownx.superfences",
        ],
        extension_configs={
            "mdx_truly_sane_lists": {
                "nested_indent": 4,
                "truly_sane": True,
            },
            "pymdownx.tasklist": {
                "clickable_checkbox": True,
            },
        },
    )

    tml = (Path(config["templates_dir"]) / "page.html").read_text(encoding="utf-8")
    html_path = (base_dir / html_filename).resolve()
    html_path.parent.mkdir(parents=True, exist_ok=True)

    output_dir = Path(config["output_dir"]).resolve()

    # ‚úÖ —á–∏–Ω–∏—Ç <img src="C:\..."> –∏ <a href="C:\..."> –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    html_content = rewrite_abs_src_href(html_content, html_path, output_dir)

    # ‚úÖ —á–∏–Ω–∏—Ç cover/icon, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ filesystem path
    page_for_template = dict(page)
    page_for_template["cover"] = to_rel_url(html_path, page.get("cover"), output_dir)
    page_for_template["icon"]  = to_rel_url(html_path, page.get("icon"),  output_dir)

    with open(html_path, "w+", encoding="utf-8") as f:
        jinja_loader = jinja2.FileSystemLoader(config["templates_dir"])
        jtemplate = jinja2.Environment(loader=jinja_loader).from_string(tml)
        html_page = jtemplate.render(content=html_content, page=page_for_template, site=structured_notion)
        f.write(html_page)



def generate_pages(structured_notion: dict, config: dict):
    # ‚úÖ –ß—Ç–æ–±—ã –æ–¥–∏–Ω —Å–ª–æ–º–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ —É–±–∏–≤–∞–ª –≤–µ—Å—å –±—ç–∫–∞–ø
    for page_id in structured_notion["pages"].keys():
        try:
            generate_page(page_id, structured_notion, config)
        except Exception as e:
            logging.error(f"ü§ñ Failed to generate page {page_id}: {e}", exc_info=True)


def generate_search_index(structured_notion: dict, config: dict):
    """Generates search index file if building for server"""
    if not config["build_locally"] and structured_notion.get("search_index"):
        out_dir = Path(config["output_dir"]).resolve()
        out_dir.mkdir(parents=True, exist_ok=True)

        search_index_path = out_dir / "search_index.json"
        with open(search_index_path, "w", encoding="utf-8") as f:
            json.dump(structured_notion["search_index"], f, ensure_ascii=False)

        # Update the search_index to just contain the path
        structured_notion["search_index"] = "search_index.json"


def generate_site(structured_notion: dict, config: dict):
    verify_templates(config)
    logging.debug("ü§ñ SASS and templates are verified.")

    generate_css(config)
    logging.debug("ü§ñ SASS translated to CSS folder.")

    generate_search_index(structured_notion, config)
    logging.debug("ü§ñ Generated search index file.")

    out_dir = Path(config["output_dir"]).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    # Fonts
    fonts_dst = out_dir / "css" / "fonts"
    if fonts_dst.exists():
        shutil.rmtree(fonts_dst)

    fonts_src = Path(config["sass_dir"]) / "fonts"
    if fonts_src.exists():
        shutil.copytree(fonts_src, fonts_dst)
        logging.debug("ü§ñ Copied fonts.")
    else:
        logging.warning("ü§ñ Fonts folder not found, skipped copying.")

    str_to_dt(structured_notion)
    logging.debug("ü§ñ Changed string in dates to datetime objects.")

    generate_archive(structured_notion, config)
    logging.info("ü§ñ Archive page generated.")

    generate_404(structured_notion, config)
    logging.info("ü§ñ 404.html page generated.")

    generate_pages(structured_notion, config)
    logging.info("ü§ñ All html and md pages generated.")
